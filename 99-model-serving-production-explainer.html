<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>99. Model Serving in Production: Scalable Inference</title>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
:root {
    --primary-color: #7B1FA2;
    --secondary-color: #8E24AA;
    --accent-color: #9C27B0;
    --bg-color: #F3E5F5;
    --card-bg: #FFFFFF;
    --text-color: #1A1A1A;
    --border-color: #CE93D8;
    --shadow: 0 2px 8px rgba(123, 31, 162, 0.1);
}
body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', sans-serif;
    background: var(--bg-color);
    color: var(--text-color);
    line-height: 1.6;
}
header {
    text-align: center;
    padding: 50px 40px;
    background: linear-gradient(135deg, #7B1FA2, #8E24AA);
    color: white;
}
header h1 {
    font-size: 2.5rem;
    font-weight: 700;
    margin-bottom: 12px;
    text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
}
.subtitle {
    font-size: 1.2rem;
    opacity: 0.95;
    font-weight: 300;
}
.author-credit {
    font-size: 1rem;
    font-weight: 600;
    margin-top: 15px;
    padding: 8px 20px;
    background: rgba(255, 255, 255, 0.25);
    border-radius: 6px;
    display: inline-block;
    text-shadow: 1px 1px 2px rgba(0,0,0,0.3);
    border: 2px solid rgba(255, 255, 255, 0.4);
}

.sidebar-wrapper {
    display: flex;
    min-height: calc(100vh - 200px);
}
.nav-menu {
    width: 280px;
    min-width: 280px;
    background: #80DEEA;
    border-right: 2px solid var(--border-color);
    padding: 20px 0;
    position: sticky;
    top: 0;
    height: calc(100vh - 200px);
    overflow-y: auto;
    display: flex;
    flex-direction: column;
    gap: 4px;
}
.nav-link {
    color: var(--text-color);
    text-decoration: none;
    padding: 12px 20px;
    margin: 0 10px;
    border-radius: 6px;
    transition: all 0.2s ease;
    font-weight: 500;
    display: block;
}
.nav-link:hover, .nav-link.active {
    background: var(--primary-color);
    color: white;
}
.main-content-wrapper {
    flex: 1;
    width: calc(100% - 280px);
    min-width: 0;
}

.section {
    padding: 40px;
    min-height: calc(100vh - 200px);
    border-bottom: 1px solid var(--border-color);
    max-width: 100%;
}
.section h2 {
    font-size: 2.2rem;
    margin-bottom: 20px;
    color: var(--primary-color);
    border-bottom: 3px solid var(--primary-color);
    padding-bottom: 10px;
}
.concept-card {
    background: var(--card-bg);
    padding: 25px;
    border-radius: 12px;
    border: 2px solid var(--border-color);
    margin: 20px 0;
    box-shadow: var(--shadow);
}
.concept-card h3 {
    color: var(--primary-color);
    margin-bottom: 15px;
    font-size: 1.4rem;
}
.mermaid { margin: 30px 0; text-align: center; }
.example-box {
    background: linear-gradient(135deg, #F3E5F5, #E1BEE7);
    border-left: 5px solid var(--primary-color);
    padding: 20px 25px;
    margin: 20px 0;
    border-radius: 8px;
}
.example-box strong {
    color: var(--primary-color);
    display: block;
    margin-bottom: 10px;
    font-size: 1.1rem;
}
footer {
    text-align: center;
    padding: 40px;
    background: #CE93D8;
    border-top: 2px solid var(--border-color);
    color: var(--text-color);
}
@media (max-width: 768px) {
    header h1 { font-size: 2rem; }
    
    .nav-menu {
        width: 100%;
        min-width: 100%;
        height: auto;
        position: relative;
        flex-direction: row;
        flex-wrap: wrap;
        padding: 10px;
        border-right: none;
        border-bottom: 2px solid var(--border-color);
    }
    .nav-link {
        padding: 8px 15px;
        margin: 4px;
    }
    
    .section { padding: 25px 20px; }
}
    .section {
    padding: 40px;
    min-height: calc(100vh - 200px);
    border-bottom: 1px solid var(--border-color);
    max-width: 100%;
}
    </style>
</head>
<body>
    <header>
        <h1>99. Model Serving in Production</h1>
        <p class="subtitle">Scalable Inference Deployment</p>
        <p class="author-credit">Explained by Naman Adep</p>
    </header>

    <div class="sidebar-wrapper">
    <nav class="nav-menu">
        <a href="#overview" class="nav-link active">Overview</a>
        <a href="#serving-patterns" class="nav-link">Serving Patterns</a>
        <a href="#scalability" class="nav-link">Scalability</a>
        <a href="#performance" class="nav-link">Performance</a>
        <a href="#reliability" class="nav-link">Reliability</a>
    </nav>
    <div class="main-content-wrapper">
        <main>
        <section id="overview" class="section">
            <h2>Model Serving in Production Overview</h2>
            <p>
                Model serving in production involves deploying trained models to serve inference requests at scale, ensuring 
                high availability, low latency, and efficient resource utilization. Production model serving requires 
                robust infrastructure, scalable architectures, performance optimization, and operational excellence. 
                Understanding production model serving enables deploying ML models that meet production requirements, 
                handling varying loads, maintaining performance, and ensuring reliability for end users.
            </p>

            <div class="mermaid">
                graph TD
                    A[Inference Requests] --> B[Load Balancer]
                    B --> C[Model Servers]
                    C --> D[Models]
                    D --> E[Responses]
                    
                    style A fill:#7B1FA2
                    style B fill:#8E24AA
                    style E fill:#9C27B0
            </div>

            <div class="example-box">
                <strong>Production Inference:</strong>
                Model serving in production handles inference requests at scale. The system receives inference requests, 
                routes them to available model instances, executes inference, and returns predictions. Production model 
                serving scales horizontally to handle varying inference loads, implements load balancing for efficiency, 
                and ensures high availability and low latency.
            </div>

            <div class="mermaid">
                mindmap
                  root((Model Serving in Production))
                    Infrastructure
                      Scalable Architecture
                      Load Balancing
                      High Availability
                    Performance
                      Low Latency
                      Efficient Resource Utilization
                      Optimization
                    Reliability
                      Fault Tolerance
                      Monitoring
                      Operational Excellence
            </div>
        </section>

        <section id="serving-patterns" class="section">
            <h2>Serving Patterns</h2>
            <p>
                Different serving patterns address various use cases, from real-time inference to batch processing, 
                each optimized for specific requirements.
            </p>

            <div class="concept-card">
                <h3>Serving Modes</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Real-Time:</strong> Real-time inference</li>
                    <li><strong>Batch:</strong> Batch inference</li>
                    <li><strong>Streaming:</strong> Streaming inference</li>
                    <li><strong>On-Demand:</strong> On-demand inference</li>
                </ul>
            </div>

            <div class="concept-card">
                <h3>Architecture Patterns</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Single Model:</strong> Single model serving</li>
                    <li><strong>Model Ensemble:</strong> Ensemble serving</li>
                    <li><strong>Pipeline:</strong> Multi-stage pipelines</li>
                    <li><strong>Microservices:</strong> Microservices architecture</li>
                </ul>
            </div>

            <div class="concept-card">
                <h3>Serving Platforms</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Triton:</strong> Triton Inference Server</li>
                    <li><strong>TensorFlow Serving:</strong> TensorFlow Serving</li>
                    <li><strong>TorchServe:</strong> PyTorch TorchServe</li>
                    <li><strong>Custom:</strong> Custom serving solutions</li>
                </ul>
            </div>
        </section>

        <section id="scalability" class="section">
            <h2>Scalability</h2>
            <p>
                Scalable model serving handles varying loads by scaling resources horizontally and vertically, ensuring 
                consistent performance under different conditions.
            </p>

            <div class="mermaid">
                graph LR
                    A[Low Load] --> B[Auto-Scaling]
                    B --> C[High Load]
                    C --> D[Scale Up]
                    D --> E[Scale Down]
                    
                    style A fill:#7B1FA2
                    style B fill:#8E24AA
                    style E fill:#9C27B0
            </div>

            <div class="concept-card">
                <h3>Scaling Strategies</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Horizontal:</strong> Scale out (add instances)</li>
                    <li><strong>Vertical:</strong> Scale up (increase resources)</li>
                    <li><strong>Auto-Scaling:</strong> Automatic scaling</li>
                    <li><strong>Load-Based:</strong> Load-based scaling</li>
                </ul>
            </div>

            <div class="concept-card">
                <h3>Load Balancing</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Round-Robin:</strong> Round-robin distribution</li>
                    <li><strong>Least Connections:</strong> Least connections</li>
                    <li><strong>Weighted:</strong> Weighted distribution</li>
                    <li><strong>Health-Based:</strong> Health-based routing</li>
                </ul>
            </div>

            <div class="concept-card">
                <h3>Resource Management</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>GPU Allocation:</strong> Efficient GPU usage</li>
                    <li><strong>Memory Management:</strong> Memory optimization</li>
                    <li><strong>Batching:</strong> Request batching</li>
                    <li><strong>Queueing:</strong> Request queueing</li>
                </ul>
            </div>
        </section>

        <section id="performance" class="section">
            <h2>Performance Optimization</h2>
            <p>
                Performance optimization ensures low latency and high throughput for model serving, maximizing resource 
                utilization and user experience.
            </p>

            <div class="concept-card">
                <h3>Optimization Techniques</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Model Optimization:</strong> Optimize models (TensorRT, quantization)</li>
                    <li><strong>Batching:</strong> Batch inference requests</li>
                    <li><strong>Caching:</strong> Cache predictions</li>
                    <li><strong>Preprocessing:</strong> Optimize preprocessing</li>
                </ul>
            </div>

            <div class="concept-card">
                <h3>Latency Optimization</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Model Size:</strong> Reduce model size</li>
                    <li><strong>Precision:</strong> Use lower precision</li>
                    <li><strong>Hardware:</strong> Optimize hardware usage</li>
                    <li><strong>Network:</strong> Optimize network</li>
                </ul>
            </div>

            <div class="concept-card">
                <h3>Throughput Optimization</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Concurrency:</strong> Increase concurrency</li>
                    <li><strong>Batching:</strong> Larger batches</li>
                    <li><strong>Pipeline:</strong> Pipeline processing</li>
                    <li><strong>Resource:</strong> Maximize resource usage</li>
                </ul>
            </div>
        </section>

        <section id="reliability" class="section">
            <h2>Reliability and Operations</h2>
            <p>
                Reliable model serving requires robust infrastructure, monitoring, error handling, and operational 
                practices to ensure consistent service availability.
            </p>

            <div class="concept-card">
                <h3>High Availability</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Redundancy:</strong> Redundant deployments</li>
                    <li><strong>Failover:</strong> Automatic failover</li>
                    <li><strong>Health Checks:</strong> Health monitoring</li>
                    <li><strong>Backup:</strong> Backup strategies</li>
                </ul>
            </div>

            <div class="concept-card">
                <h3>Error Handling</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Retries:</strong> Request retries</li>
                    <li><strong>Fallbacks:</strong> Fallback models</li>
                    <li><strong>Circuit Breakers:</strong> Circuit breaker pattern</li>
                    <li><strong>Graceful Degradation:</strong> Graceful degradation</li>
                </ul>
            </div>

            <div class="concept-card">
                <h3>Monitoring</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Metrics:</strong> Performance metrics</li>
                    <li><strong>Logging:</strong> Request logging</li>
                    <li><strong>Alerting:</strong> Alert on issues</li>
                    <li><strong>Dashboards:</strong> Monitoring dashboards</li>
                </ul>
            </div>

            <div class="example-box">
                <strong>Model Serving in Production:</strong>
                Deploy models for production serving by choosing appropriate serving patterns, implementing scalable 
                architectures, optimizing performance, and ensuring reliability. Use serving platforms like Triton, 
                implement auto-scaling, optimize for latency and throughput, and monitor production deployments. 
                Production model serving enables reliable, scalable inference that meets production requirements.
            </div>
        </section>
            </main>

            </div>
</div>

    <footer>
        <p>Model Serving in Production Guide | Scalable Inference</p>
    </footer>

    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
        
        document.querySelectorAll('.nav-link').forEach(link => {
            link.addEventListener('click', function(e) {
                e.preventDefault();
                const targetId = this.getAttribute('href');
                const targetSection = document.querySelector(targetId);
                if (targetSection) {
                    targetSection.scrollIntoView({ behavior: 'smooth' }
);
                    document.querySelectorAll('.nav-link').forEach(l => l.classList.remove('active'));
                    this.classList.add('active');
                }

            });
        });

        const sections = document.querySelectorAll('.section');
        const navLinks = document.querySelectorAll('.nav-link');
        
        window.addEventListener('scroll', () => {
            let current = '';
            sections.forEach(section => {
                const sectionTop = section.offsetTop;
                if (pageYOffset >= sectionTop - 200) {
                    current = section.getAttribute('id');
                }

            });
            
            navLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === `#${current}`) {
                    link.classList.add('active');
                }
            });
        });
    </script>
</body>
</html>
