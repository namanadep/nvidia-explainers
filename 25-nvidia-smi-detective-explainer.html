<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>nvidia-smi: GPU Monitoring and Management Command-Line Tool</title>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
:root {
    --primary-color: #1a1a1a;
    --secondary-color: #4a4a4a;
    --accent-color: #d4af37;
    --bg-color: #f5f5f5;
    --card-bg: #ffffff;
    --text-color: #1a1a1a;
    --text-muted: #6c757d;
    --border-color: #ddd;
    --shadow: 0 2px 8px rgba(0,0,0,0.1);
}
body {
    font-family: 'Courier New', monospace;
    background: var(--bg-color);
    color: var(--text-color);
    line-height: 1.6;
}
header {
    text-align: center;
    padding: 50px 40px;
    background: linear-gradient(135deg, #1a1a1a, #2d2d2d);
    color: #d4af37;
    position: relative;
    border-bottom: 3px solid #d4af37;
}
header::before {
    content: '';
    position: absolute;
    width: 100%;
    height: 100%;
    background: radial-gradient(circle at 50% 50%, rgba(212, 175, 55, 0.1) 0%, transparent 70%);
    top: 0;
    left: 0;
}
header h1 {
    font-size: 2.5rem;
    font-weight: 700;
    margin-bottom: 12px;
    text-shadow: 2px 2px 4px rgba(0,0,0,0.5);
    font-family: 'Courier New', monospace;
}
.sidebar-wrapper {
    display: flex;
    min-height: calc(100vh - 200px);
}
.nav-menu {
    width: 280px;
    min-width: 280px;
    background: #80DEEA;
    border-right: 2px solid var(--border-color);
    padding: 20px 0;
    position: sticky;
    top: 0;
    height: calc(100vh - 200px);
    overflow-y: auto;
    display: flex;
    flex-direction: column;
    gap: 4px;
}
.nav-link {
    color: var(--text-color);
    text-decoration: none;
    padding: 12px 20px;
    margin: 0 10px;
    border-radius: 6px;
    transition: all 0.2s ease;
    font-weight: 500;
    display: block;
}
.nav-link:hover, .nav-link.active {
    background: #d4af37;
    color: #1a1a1a;
    border-color: #d4af37;
}
.main-content-wrapper {
    flex: 1;
    width: calc(100% - 280px);
    min-width: 0;
}

.section {
    padding: 40px;
    min-height: calc(100vh - 200px);
    border-bottom: 1px solid var(--border-color);
    max-width: 100%;
}
.section h2 {
    font-size: 2rem;
    margin-bottom: 20px;
    color: var(--primary-color);
    border-bottom: 3px solid var(--accent-color);
    padding-bottom: 10px;
    font-family: 'Courier New', monospace;
}
.case-file {
    background: #f9f9f9;
    border-left: 4px solid var(--accent-color);
    padding: 20px;
    margin: 20px 0;
    border-radius: 4px;
    font-family: 'Courier New', monospace;
}
.command-box {
    background: #1a1a1a;
    color: #00ff00;
    padding: 15px;
    border-radius: 4px;
    margin: 15px 0;
    font-family: 'Courier New', monospace;
    overflow-x: auto;
}
.cheat-sheet {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
    gap: 20px;
    margin: 30px 0;
}
.cheat-card {
    background: var(--card-bg);
    padding: 20px;
    border: 2px solid var(--accent-color);
    border-radius: 4px;
    box-shadow: var(--shadow);
}
.mermaid { margin: 30px 0; text-align: center; }
footer {
    text-align: center;
    padding: 40px;
    background: #2d2d2d;
    color: #d4af37;
    border-top: 3px solid #d4af37;
}
@media (max-width: 768px) {
    header h1 { font-size: 2rem; }
    
    .nav-menu {
        width: 100%;
        min-width: 100%;
        height: auto;
        position: relative;
        flex-direction: row;
        flex-wrap: wrap;
        padding: 10px;
        border-right: none;
        border-bottom: 2px solid var(--border-color);
    }
    .nav-link {
        padding: 8px 15px;
        margin: 4px;
    }
    
    .section { padding: 25px 20px; }
}
    .section {
    padding: 40px;
    min-height: calc(100vh - 200px);
    border-bottom: 1px solid var(--border-color);
    max-width: 100%;
}
    .cheat-sheet { grid-template-columns: 1fr; }
    
.author-credit {
    font-size: 1rem;
    font-weight: 600;
    margin-top: 15px;
    padding: 8px 20px;
    background: rgba(255, 255, 255, 0.25);
    border-radius: 6px;
    display: inline-block;
    text-shadow: 1px 1px 2px rgba(0,0,0,0.3);
    border: 2px solid rgba(255, 255, 255, 0.4);
}
</style>
</head>
<body>
    <header>
        <h1>25. nvidia-smi: GPU Monitoring and Management</h1>
        <p class="subtitle">Comprehensive Command-Line Interface for NVIDIA GPU Diagnostics and Control</p>
        <p class="author-credit">Explained by Naman Adep</p>
    </header>

    <div class="sidebar-wrapper">
    <nav class="nav-menu">
        <a href="#overview" class="nav-link active">Overview</a>
        <a href="#monitoring" class="nav-link">Monitoring</a>
        <a href="#troubleshooting" class="nav-link">Troubleshooting</a>
        <a href="#commands" class="nav-link">Commands</a>
        <a href="#advanced" class="nav-link">Advanced Usage</a>
        <a href="#reference" class="nav-link">Reference</a>
    </nav>
    <div class="main-content-wrapper">
        <main>
        <section id="overview" class="section">
            <h2>nvidia-smi Overview and Capabilities</h2>
            <p>
                nvidia-smi (NVIDIA System Management Interface) is a command-line utility that provides comprehensive 
                monitoring and management capabilities for NVIDIA GPUs. It enables administrators and developers to 
                query GPU status, monitor performance metrics, manage GPU configurations, and diagnose issues in 
                real-time.
            </p>

            <div class="mermaid">
                graph TD
                    A[nvidia-smi] --> B[Utilization<br/>How busy is GPU?]
                    A --> C[Memory<br/>How much used?]
                    A --> D[Temperature<br/>Is it overheating?]
                    A --> E[Power<br/>How much consumed?]
                    A --> F[Processes<br/>What's running?]
                    
                    style A fill:#1a1a1a
                    style B fill:#d4af37
                    style C fill:#d4af37
                    style D fill:#d4af37
                    style E fill:#d4af37
                    style F fill:#d4af37
            </div>

            <div class="case-file">
                <strong>Basic Command: Standard Output</strong>
                <div class="command-box">$ nvidia-smi</div>
                <p>The standard nvidia-smi command provides a comprehensive overview of all GPUs in the system, displaying 
                utilization metrics, memory usage, temperature, power consumption, and active processes. This single command 
                offers immediate visibility into GPU health and performance status.</p>
            </div>

            <div class="mermaid">
                mindmap
                  root((nvidia-smi))
                    Monitoring
                      GPU Utilization
                      Memory Usage
                      Temperature
                      Power Consumption
                    Process Management
                      Active Processes
                      Process Details
                      Resource Allocation
                    Diagnostics
                      GPU Status
                      Driver Information
                      Error Detection
                    Advanced Features
                      Query Options
                      Formatting
                      Continuous Monitoring
            </div>

            <div class="mermaid">
                flowchart TD
                    A[GPU Issue] --> B{Run nvidia-smi}
                    B --> C{Check Status}
                    C -->|Utilization| D[Check GPU Usage]
                    C -->|Memory| E[Check Memory Usage]
                    C -->|Temperature| F[Check Temperature]
                    C -->|Processes| G[Check Active Processes]
                    
                    D --> H[Identify Issue]
                    E --> H
                    F --> H
                    G --> H
                    
                    H --> I[Take Action]
                    
                    style A fill:#1a1a1a
                    style H fill:#d4af37
            </div>
        </section>

        <section id="monitoring" class="section">
            <h2>Monitoring Capabilities and Command Options</h2>
            <p>
                nvidia-smi provides extensive command-line options for targeted monitoring and querying of specific GPU 
                attributes. These options enable precise data extraction for automation, logging, and performance analysis.
            </p>

            <div class="cheat-sheet">
                <div class="cheat-card">
                    <h3>--query-gpu</h3>
                    <p><strong>Purpose:</strong> Targeted Query Interface</p>
                    <p>Query specific GPU attributes with precise formatting options. Enables programmatic data extraction 
                    for monitoring scripts and performance analysis tools.</p>
                    <div class="command-box" style="font-size: 0.9rem;">
                        nvidia-smi --query-gpu=utilization.gpu --format=csv
                    </div>
                </div>
                <div class="cheat-card">
                    <h3>-l (loop)</h3>
                    <p><strong>Purpose:</strong> Continuous Monitoring</p>
                    <p>Enable periodic monitoring with configurable update intervals. Essential for tracking performance 
                    trends and detecting anomalies over time.</p>
                    <div class="command-box" style="font-size: 0.9rem;">
                        nvidia-smi -l 2
                    </div>
                </div>
                <div class="cheat-card">
                    <h3>-i (index)</h3>
                    <p><strong>Purpose:</strong> GPU Selection</p>
                    <p>Target specific GPU devices in multi-GPU systems. Critical for systems with multiple GPUs where 
                    individual device monitoring is required.</p>
                    <div class="command-box" style="font-size: 0.9rem;">
                        nvidia-smi -i 0
                    </div>
                </div>
            </div>
        </section>

        <section id="troubleshooting" class="section">
            <h2>Common Troubleshooting Scenarios</h2>
            <p>
                nvidia-smi is essential for diagnosing GPU performance issues and system problems. The following scenarios 
                demonstrate practical applications of nvidia-smi for identifying and resolving common GPU-related issues.
            </p>

            <div class="case-file">
                <strong>Scenario: Low Training Performance</strong>
                <p><strong>Diagnosis:</strong> Monitor GPU utilization to identify underutilization</p>
                <div class="command-box">
                    nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader
                </div>
                <p><strong>Analysis:</strong> GPU utilization below 50% indicates inefficient GPU usage. Potential causes 
                include CPU bottlenecks, data loading inefficiencies, or suboptimal batch sizes. Investigate data pipeline 
                performance and model architecture for optimization opportunities.</p>
            </div>

            <div class="case-file">
                <strong>Scenario: Out of Memory Errors</strong>
                <p><strong>Diagnosis:</strong> Monitor memory consumption patterns</p>
                <div class="command-box">
                    nvidia-smi --query-gpu=memory.used,memory.total --format=csv
                </div>
                <p><strong>Analysis:</strong> Memory utilization approaching 100% indicates insufficient GPU memory for the 
                current workload. Solutions include reducing batch size, implementing gradient checkpointing, using mixed 
                precision training, or upgrading to GPUs with larger memory capacity.</p>
            </div>

            <div class="case-file">
                <strong>Scenario: Thermal Throttling</strong>
                <p><strong>Diagnosis:</strong> Monitor GPU temperature and power consumption</p>
                <div class="command-box">
                    nvidia-smi --query-gpu=temperature.gpu --format=csv
                </div>
                <p><strong>Evidence:</strong> Normal is 60-80Â°C. Above 85Â°C is concerning. Check cooling and workload.</p>
            </div>

            <div class="case-file">
                <strong>Case: "Power consumption spike"</strong>
                <p><strong>Investigation:</strong> Check power draw</p>
                <div class="command-box">
                    nvidia-smi --query-gpu=power.draw,power.limit --format=csv
                </div>
                <p><strong>Evidence:</strong> Compare draw vs limit. High power draw means intensive workload or 
                inefficient code.</p>
            </div>
        </section>

        <section id="evidence" class="section">
            <h2>ðŸ“Š Reading the Evidence: Understanding Output</h2>
            <p>
                Understanding nvidia-smi output is essential for diagnosing GPU performance issues and 
                optimizing workloads.
            </p>

            <div class="mermaid">
                graph LR
                    A[nvidia-smi Output] --> B[GPU Name<br/>What model?]
                    A --> C[Utilization %<br/>How busy?]
                    A --> D[Memory Usage<br/>How much used?]
                    A --> E[Temperature<br/>How hot?]
                    A --> F[Power Draw<br/>How much power?]
                    
                    style A fill:#1a1a1a
            </div>

            <div class="mermaid">
                flowchart TD
                    A[nvidia-smi Output] --> B{Check Metric}
                    B -->|Utilization| C{Value?}
                    B -->|Memory| D{Value?}
                    B -->|Temperature| E{Value?}
                    B -->|Power| F{Value?}
                    
                    C -->|0%| G[GPU Idle<br/>Nothing Running]
                    C -->|100%| H[GPU Maxed<br/>Possible Bottleneck]
                    D -->|95%+| I[Memory Full<br/>Reduce Batch Size]
                    E -->|85Â°C+| J[Overheating<br/>Check Cooling]
                    F -->|At Limit| K[Max Power<br/>Intensive Workload]
                    
                    style A fill:#1a1a1a
                    style G fill:#d4af37
                    style H fill:#d4af37
            </div>

            <div class="mermaid">
                flowchart TD
                    A[GPU Metrics] --> B[Utilization 0%<br/>GPU Idle]
                    A --> C[Utilization 100%<br/>GPU Maxed]
                    A --> D[Memory 95%+<br/>Out of Memory]
                    A --> E[Temperature 85Â°C+<br/>Overheating]
                    A --> F[Power at Limit<br/>Max Power]
                    
                    B --> G[Case Closed<br/>Nothing Running]
                    C --> H[Possible Bottleneck<br/>Check Workload]
                    D --> I[Reduce Batch Size<br/>Optimize Memory]
                    E --> J[Check Cooling<br/>Thermal Issue]
                    F --> K[Intensive Workload<br/>Normal or Inefficient]
                    
                    style A fill:#1a1a1a
                    style G fill:#d4af37
                </div>
        </section>

        <section id="monitoring" class="section">
            <h2>Continuous Monitoring Mode</h2>
            <p>
                The -l flag enables continuous monitoring, updating nvidia-smi output at regular intervals 
                to track GPU status over time.
            </p>

            <div class="command-box">
                nvidia-smi -l 2
            </div>
            <p style="margin-top: 15px;">
                This updates every 2 seconds, providing continuous visibility into GPU status. Useful for 
                monitoring training progress, detecting memory leaks, or tracking performance metrics over time.
            </p>

            <div class="mermaid">
                flowchart TD
                    A[Continuous Monitoring] --> B[Training Progress<br/>Real-Time Tracking]
                    A --> C[Memory Leaks<br/>Detect Gradual Increase]
                    A --> D[Performance Tracking<br/>Long Runs]
                    A --> E[Thermal Monitoring<br/>Throttling Detection]
                    
                    B --> F[Complete Monitoring<br/>System Health]
                    C --> F
                    D --> F
                    E --> F
                    
                    style A fill:#1a1a1a
                    style F fill:#d4af37
                </div>
        </section>

        <section id="cheat-sheet" class="section">
            <h2>ðŸ“‹ The Detective's Cheat Sheet</h2>
            <p>
                Quick reference for common investigations and their solutions.
            </p>

            <div class="cheat-sheet">
                <div class="cheat-card">
                    <h3>Slow Training</h3>
                    <p><strong>Investigation:</strong></p>
                    <div class="command-box" style="font-size: 0.85rem;">
                        nvidia-smi --query-gpu=utilization.gpu
                    </div>
                    <p><strong>Solution:</strong> If low, optimize code for GPU. If high, check data pipeline.</p>
                </div>
                <div class="cheat-card">
                    <h3>Out of Memory</h3>
                    <p><strong>Investigation:</strong></p>
                    <div class="command-box" style="font-size: 0.85rem;">
                        nvidia-smi --query-gpu=memory.used,memory.total
                    </div>
                    <p><strong>Solution:</strong> Reduce batch size, use gradient checkpointing, or use larger GPU.</p>
                </div>
                <div class="cheat-card">
                    <h3>Overheating</h3>
                    <p><strong>Investigation:</strong></p>
                    <div class="command-box" style="font-size: 0.85rem;">
                        nvidia-smi --query-gpu=temperature.gpu
                    </div>
                    <p><strong>Solution:</strong> Check cooling, reduce workload, or check for dust/obstructions.</p>
                </div>
                <div class="cheat-card">
                    <h3>High Power</h3>
                    <p><strong>Investigation:</strong></p>
                    <div class="command-box" style="font-size: 0.85rem;">
                        nvidia-smi --query-gpu=power.draw,power.limit
                    </div>
                    <p><strong>Solution:</strong> Normal for intensive workloads. If unexpected, check for inefficient code.</p>
                </div>
            </div>
        </section>
            </main>

            </div>
</div>

    <footer>
        <p><strong>nvidia-smi: The GPU Detective</strong> - Solving Mysteries, One Command at a Time</p>
        <p>Case closed. GPU mysteries solved. </p>
    </footer>

    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'dark' });
        document.querySelectorAll('.nav-link').forEach(link => {
            link.addEventListener('click', function(e) {
                e.preventDefault();
                document.querySelectorAll('.nav-link').forEach(l => l.classList.remove('active'));
                this.classList.add('active');
                document.querySelector(this.getAttribute('href')).scrollIntoView({ behavior: 'smooth' }
);
            });
        });
        window.addEventListener('scroll', () => {
            const sections = document.querySelectorAll('.section');
            const navLinks = document.querySelectorAll('.nav-link');
            let current = '';
            sections.forEach(s => {
                if (window.pageYOffset >= s.offsetTop - 100) current = s.getAttribute('id');
            }
);
            navLinks.forEach(l => {
                l.classList.remove('active');
                if (l.getAttribute('href') === `#${current}`) l.classList.add('active');
            });
        });
    </script>
</body>
</html>
