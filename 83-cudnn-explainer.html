<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>83. cuDNN: Deep Learning Acceleration Library</title>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
:root {
    --primary-color: #00695C;
    --secondary-color: #00897B;
    --accent-color: #26A69A;
    --bg-color: #E0F2F1;
    --card-bg: #FFFFFF;
    --text-color: #1A1A1A;
    --border-color: #80CBC4;
    --shadow: 0 2px 8px rgba(0, 105, 92, 0.1);
}
body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', sans-serif;
    background: var(--bg-color);
    color: var(--text-color);
    line-height: 1.6;
}
header {
    text-align: center;
    padding: 50px 40px;
    background: linear-gradient(135deg, #00695C, #00897B);
    color: white;
}
header h1 {
    font-size: 2.5rem;
    font-weight: 700;
    margin-bottom: 12px;
    text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
}
.subtitle {
    font-size: 1.2rem;
    opacity: 0.95;
    font-weight: 300;
}
.author-credit {
    font-size: 1rem;
    font-weight: 600;
    margin-top: 15px;
    padding: 8px 20px;
    background: rgba(255, 255, 255, 0.25);
    border-radius: 6px;
    display: inline-block;
    text-shadow: 1px 1px 2px rgba(0,0,0,0.3);
    border: 2px solid rgba(255, 255, 255, 0.4);
}

.sidebar-wrapper {
    display: flex;
    min-height: calc(100vh - 200px);
}
.nav-menu {
    width: 280px;
    min-width: 280px;
    background: #80DEEA;
    border-right: 2px solid var(--border-color);
    padding: 20px 0;
    position: sticky;
    top: 0;
    height: calc(100vh - 200px);
    overflow-y: auto;
    display: flex;
    flex-direction: column;
    gap: 4px;
}
.nav-link {
    color: var(--text-color);
    text-decoration: none;
    padding: 12px 20px;
    margin: 0 10px;
    border-radius: 6px;
    transition: all 0.2s ease;
    font-weight: 500;
    display: block;
}
.nav-link:hover, .nav-link.active {
    background: var(--primary-color);
    color: white;
}
.main-content-wrapper {
    flex: 1;
    width: calc(100% - 280px);
    min-width: 0;
}

.section {
    padding: 40px;
    min-height: calc(100vh - 200px);
    border-bottom: 1px solid var(--border-color);
    max-width: 100%;
}
.section h2 {
    font-size: 2.2rem;
    margin-bottom: 20px;
    color: var(--primary-color);
    border-bottom: 3px solid var(--primary-color);
    padding-bottom: 10px;
}
.concept-card {
    background: var(--card-bg);
    padding: 25px;
    border-radius: 12px;
    border: 2px solid var(--border-color);
    margin: 20px 0;
    box-shadow: var(--shadow);
}
.concept-card h3 {
    color: var(--primary-color);
    margin-bottom: 15px;
    font-size: 1.4rem;
}
.mermaid { margin: 30px 0; text-align: center; }
.example-box {
    background: linear-gradient(135deg, #E0F2F1, #B2DFDB);
    border-left: 5px solid var(--primary-color);
    padding: 20px 25px;
    margin: 20px 0;
    border-radius: 8px;
}
.example-box strong {
    color: var(--primary-color);
    display: block;
    margin-bottom: 10px;
    font-size: 1.1rem;
}
footer {
    text-align: center;
    padding: 40px;
    background: #80CBC4;
    border-top: 2px solid var(--border-color);
    color: var(--text-color);
}
@media (max-width: 768px) {
    header h1 { font-size: 2rem; }
    
    .nav-menu {
        width: 100%;
        min-width: 100%;
        height: auto;
        position: relative;
        flex-direction: row;
        flex-wrap: wrap;
        padding: 10px;
        border-right: none;
        border-bottom: 2px solid var(--border-color);
    }
    .nav-link {
        padding: 8px 15px;
        margin: 4px;
    }
    
    .section { padding: 25px 20px; }
}
    .section {
    padding: 40px;
    min-height: calc(100vh - 200px);
    border-bottom: 1px solid var(--border-color);
    max-width: 100%;
}
    </style>
</head>
<body>
    <header>
        <h1>83. cuDNN</h1>
        <p class="subtitle">Deep Learning Acceleration Library</p>
        <p class="author-credit">Explained by Naman Adep</p>
    </header>

    <div class="sidebar-wrapper">
    <nav class="nav-menu">
        <a href="#overview" class="nav-link active">Overview</a>
        <a href="#features" class="nav-link">Features</a>
        <a href="#optimization" class="nav-link">Optimization</a>
        <a href="#integration" class="nav-link">Framework Integration</a>
        <a href="#performance" class="nav-link">Performance Tuning</a>
    </nav>
    <div class="main-content-wrapper">
        <main>
        <section id="overview" class="section">
            <h2>cuDNN Overview</h2>
            <p>
                cuDNN (CUDA Deep Neural Network library) is NVIDIA's GPU-accelerated library for deep learning primitives. 
                It provides highly optimized implementations of common deep learning operations such as convolutions, 
                pooling, normalization, and activation functions. cuDNN is designed to be integrated into deep learning 
                frameworks, enabling them to leverage GPU acceleration automatically. The library is optimized for various 
                GPU architectures and provides the performance foundation for training and inference of deep neural networks.
            </p>

            <div class="mermaid">
                graph TD
                    A[cuDNN Library] --> B[Convolutions]
                    A --> C[Pooling]
                    A --> D[Normalization]
                    A --> E[Activation Functions]
                    A --> F[RNN Operations]
                    
                    style A fill:#00695C
                    style B fill:#00897B
                    style C fill:#26A69A
            </div>

            <div class="example-box">
                <strong>Deep Learning Acceleration:</strong>
                cuDNN is a specialized library for deep learning operations, providing highly optimized implementations 
                of common operations like convolutions, pooling, normalization, and activation functions. Deep learning 
                frameworks integrate cuDNN to accelerate training and inference on NVIDIA GPUs, providing significant 
                performance improvements while maintaining framework compatibility.
            </div>

            <div class="mermaid">
                mindmap
                  root((cuDNN))
                    Operations
                      Convolutions
                      Pooling
                      Normalization
                      Activation Functions
                      RNN Operations
                    Integration
                      Framework Integration
                      Automatic Acceleration
                    Benefits
                      Performance Improvements
                      Framework Compatibility
                      GPU Optimization
            </div>
        </section>

        <section id="features" class="section">
            <h2>cuDNN Features</h2>
            <p>
                cuDNN provides optimized implementations of essential deep learning operations, each tuned for maximum 
                performance on NVIDIA GPUs.
            </p>

            <div class="concept-card">
                <h3>Core Operations</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Convolutions:</strong> 2D and 3D convolutions with various algorithms</li>
                    <li><strong>Pooling:</strong> Max, average, and other pooling operations</li>
                    <li><strong>Normalization:</strong> Batch normalization, layer normalization</li>
                    <li><strong>Activation:</strong> ReLU, sigmoid, tanh, and other activations</li>
                    <li><strong>RNN:</strong> Recurrent neural network operations</li>
                </ul>
            </div>

            <div class="concept-card">
                <h3>Advanced Features</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Tensor Cores:</strong> Leverages Tensor Cores for mixed precision</li>
                    <li><strong>Multi-GPU:</strong> Supports multi-GPU operations</li>
                    <li><strong>Algorithm Selection:</strong> Automatic algorithm selection</li>
                    <li><strong>Workspace Management:</strong> Efficient memory management</li>
                </ul>
            </div>

            <div class="concept-card">
                <h3>Precision Support</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>FP32:</strong> Single precision floating point</li>
                    <li><strong>FP16:</strong> Half precision for Tensor Cores</li>
                    <li><strong>INT8:</strong> Integer precision for inference</li>
                    <li><strong>Mixed Precision:</strong> Automatic mixed precision</li>
                </ul>
            </div>
        </section>

        <section id="optimization" class="section">
            <h2>cuDNN Optimization</h2>
            <p>
                cuDNN is highly optimized for NVIDIA GPU architectures, providing automatic algorithm selection and 
                performance tuning.
            </p>

            <div class="mermaid">
                graph LR
                    A[Operation Request] --> B[cuDNN]
                    B --> C[Algorithm Selection]
                    C --> D[Optimized Execution]
                    D --> E[GPU Acceleration]
                    
                    style A fill:#00695C
                    style B fill:#00897B
                    style E fill:#26A69A
            </div>

            <div class="concept-card">
                <h3>Algorithm Selection</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Automatic:</strong> Automatic algorithm selection</li>
                    <li><strong>Heuristics:</strong> Performance-based heuristics</li>
                    <li><strong>Custom:</strong> Manual algorithm selection</li>
                    <li><strong>Benchmarking:</strong> Algorithm benchmarking</li>
                </ul>
            </div>

            <div class="concept-card">
                <h3>Performance Optimizations</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Memory Layout:</strong> Optimized memory layouts</li>
                    <li><strong>Kernel Fusion:</strong> Fused operations</li>
                    <li><strong>Tensor Cores:</strong> Tensor Core utilization</li>
                    <li><strong>Workspace Reuse:</strong> Efficient workspace management</li>
                </ul>
            </div>

            <div class="concept-card">
                <h3>Architecture Optimization</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>GPU-Specific:</strong> Optimized for each GPU architecture</li>
                    <li><strong>Compute Capability:</strong> Leverages compute capability</li>
                    <li><strong>Memory Bandwidth:</strong> Optimized memory access</li>
                    <li><strong>SM Count:</strong> Scales with SM count</li>
                </ul>
            </div>
        </section>

        <section id="integration" class="section">
            <h2>Framework Integration</h2>
            <p>
                cuDNN integrates seamlessly with popular deep learning frameworks, providing GPU acceleration automatically 
                without requiring framework-specific code.
            </p>

            <div class="concept-card">
                <h3>Supported Frameworks</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>TensorFlow:</strong> Native cuDNN integration</li>
                    <li><strong>PyTorch:</strong> Built-in cuDNN support</li>
                    <li><strong>Caffe:</strong> cuDNN backend</li>
                    <li><strong>MXNet:</strong> cuDNN operations</li>
                    <li><strong>Others:</strong> Many frameworks support cuDNN</li>
                </ul>
            </div>

            <div class="concept-card">
                <h3>Integration Benefits</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Transparent:</strong> Transparent to framework users</li>
                    <li><strong>Automatic:</strong> Automatic GPU acceleration</li>
                    <li><strong>Optimized:</strong> Optimized performance</li>
                    <li><strong>Compatible:</strong> Framework compatibility</li>
                </ul>
            </div>

            <div class="concept-card">
                <h3>Version Compatibility</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>CUDA Version:</strong> Match CUDA version</li>
                    <li><strong>Framework Version:</strong> Framework compatibility</li>
                    <li><strong>GPU Architecture:</strong> GPU compute capability</li>
                    <li><strong>Updates:</strong> Regular updates for new features</li>
                </ul>
            </div>
        </section>

        <section id="performance" class="section">
            <h2>Performance Tuning</h2>
            <p>
                Optimizing cuDNN performance involves selecting appropriate algorithms, managing workspace memory, and 
                leveraging GPU-specific features.
            </p>

            <div class="concept-card">
                <h3>Tuning Strategies</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Algorithm Selection:</strong> Choose optimal algorithms</li>
                    <li><strong>Workspace Size:</strong> Allocate sufficient workspace</li>
                    <li><strong>Precision:</strong> Use appropriate precision</li>
                    <li><strong>Tensor Cores:</strong> Enable Tensor Core operations</li>
                </ul>
            </div>

            <div class="concept-card">
                <h3>Best Practices</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Latest Version:</strong> Use latest cuDNN version</li>
                    <li><strong>Benchmarking:</strong> Benchmark algorithm choices</li>
                    <li><strong>Memory Management:</strong> Efficient workspace management</li>
                    <li><strong>Profiling:</strong> Profile cuDNN operations</li>
                </ul>
            </div>

            <div class="concept-card">
                <h3>Performance Monitoring</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Nsight Tools:</strong> Profile with Nsight</li>
                    <li><strong>Framework Profilers:</strong> Use framework profilers</li>
                    <li><strong>Metrics:</strong> Monitor performance metrics</li>
                    <li><strong>Optimization:</strong> Iterate on optimization</li>
                </ul>
            </div>

            <div class="example-box">
                <strong>cuDNN Usage:</strong>
                cuDNN provides the performance foundation for deep learning frameworks. Frameworks automatically use 
                cuDNN for GPU-accelerated operations, providing optimized performance without requiring manual 
                optimization. Use latest cuDNN versions, ensure proper installation, and leverage Tensor Cores for 
                mixed precision operations. cuDNN enables efficient training and inference of deep neural networks on 
                NVIDIA GPUs.
            </div>
        </section>
            </main>

            </div>
</div>

    <footer>
        <p>cuDNN Guide | Deep Learning Acceleration Library</p>
    </footer>

    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
        
        document.querySelectorAll('.nav-link').forEach(link => {
            link.addEventListener('click', function(e) {
                e.preventDefault();
                const targetId = this.getAttribute('href');
                const targetSection = document.querySelector(targetId);
                if (targetSection) {
                    targetSection.scrollIntoView({ behavior: 'smooth' }
);
                    document.querySelectorAll('.nav-link').forEach(l => l.classList.remove('active'));
                    this.classList.add('active');
                }

            });
        });

        const sections = document.querySelectorAll('.section');
        const navLinks = document.querySelectorAll('.nav-link');
        
        window.addEventListener('scroll', () => {
            let current = '';
            sections.forEach(section => {
                const sectionTop = section.offsetTop;
                if (pageYOffset >= sectionTop - 200) {
                    current = section.getAttribute('id');
                }

            });
            
            navLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === `#${current}`) {
                    link.classList.add('active');
                }
            });
        });
    </script>
</body>
</html>
