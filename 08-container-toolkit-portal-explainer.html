<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NVIDIA Container Toolkit: GPU Access for Containerized Applications</title>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
:root {
    --primary-color: #7B1FA2;
    --secondary-color: #9C27B0;
    --accent-color: #BA68C8;
    --bg-color: #F3E5F5;
    --card-bg: #FFFFFF;
    --text-color: #1A1A1A;
    --border-color: #CE93D8;
    --shadow: 0 2px 8px rgba(123, 31, 162, 0.1);
}
body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', sans-serif;
    background: var(--bg-color);
    color: var(--text-color);
    line-height: 1.6;
}
header {
    text-align: center;
    padding: 50px 40px;
    background: linear-gradient(135deg, #7B1FA2, #9C27B0);
    color: white;
    position: relative;
}
header::before {
    content: '';
    position: absolute;
    font-size: 200px;
    opacity: 0.1;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
    animation: sparkle 3s ease-in-out infinite;
}
@keyframes sparkle {
    0%, 100% { opacity: 0.1; transform: translate(-50%, -50%) scale(1); }
    50% { opacity: 0.3; transform: translate(-50%, -50%) scale(1.2); }
}
header h1 {
    font-size: 2.5rem;
    font-weight: 700;
    margin-bottom: 12px;
    text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
}
.sidebar-wrapper {
    display: flex;
    min-height: calc(100vh - 200px);
}
.nav-menu {
    width: 280px;
    min-width: 280px;
    background: #80DEEA;
    border-right: 2px solid var(--border-color);
    padding: 20px 0;
    position: sticky;
    top: 0;
    height: calc(100vh - 200px);
    overflow-y: auto;
    display: flex;
    flex-direction: column;
    gap: 4px;
}
.nav-link {
    color: var(--text-color);
    text-decoration: none;
    padding: 12px 20px;
    margin: 0 10px;
    border-radius: 6px;
    transition: all 0.2s ease;
    font-weight: 500;
    display: block;
}
.nav-link:hover, .nav-link.active {
    background: var(--primary-color);
    color: white;
}
.main-content-wrapper {
    flex: 1;
    width: calc(100% - 280px);
    min-width: 0;
}

.section {
    padding: 40px;
    min-height: calc(100vh - 200px);
    border-bottom: 1px solid var(--border-color);
    max-width: 100%;
}
.section h2 {
    font-size: 2.2rem;
    margin-bottom: 20px;
    color: var(--primary-color);
    border-bottom: 3px solid var(--primary-color);
    padding-bottom: 10px;
}
.portal-card {
    background: var(--card-bg);
    padding: 25px;
    border-radius: 12px;
    border: 2px solid var(--border-color);
    margin: 20px 0;
    box-shadow: var(--shadow);
}
.mermaid { margin: 30px 0; text-align: center; }
.example-box {
    background: linear-gradient(135deg, #F3E5F5, #CE93D8);
    border-left: 5px solid var(--primary-color);
    padding: 20px 25px;
    margin: 20px 0;
    border-radius: 8px;
}
footer {
    text-align: center;
    padding: 40px;
    background: #CE93D8;
    border-top: 2px solid var(--border-color);
    color: var(--text-color);
}
@media (max-width: 768px) {
    header h1 { font-size: 2rem; }
    
    .nav-menu {
        width: 100%;
        min-width: 100%;
        height: auto;
        position: relative;
        flex-direction: row;
        flex-wrap: wrap;
        padding: 10px;
        border-right: none;
        border-bottom: 2px solid var(--border-color);
    }
    .nav-link {
        padding: 8px 15px;
        margin: 4px;
    }
    
    .section { padding: 25px 20px; }
}
    .section {
    padding: 40px;
    min-height: calc(100vh - 200px);
    border-bottom: 1px solid var(--border-color);
    max-width: 100%;
}
    
.author-credit {
    font-size: 1rem;
    font-weight: 600;
    margin-top: 15px;
    padding: 8px 20px;
    background: rgba(255, 255, 255, 0.25);
    border-radius: 6px;
    display: inline-block;
    text-shadow: 1px 1px 2px rgba(0,0,0,0.3);
    border: 2px solid rgba(255, 255, 255, 0.4);
}
</style>
</head>
<body>
    <header>
        <h1>8. NVIDIA Container Toolkit</h1>
        <p class="subtitle">Enabling GPU Access for Containerized AI and HPC Workloads</p>
        <p class="author-credit">Explained by Naman Adep</p>
    </header>

    <div class="sidebar-wrapper">
    <nav class="nav-menu">
        <a href="#overview" class="nav-link active">Overview</a>
        <a href="#containers" class="nav-link">Standard Containers</a>
        <a href="#toolkit" class="nav-link">Container Toolkit</a>
        <a href="#gpu-access" class="nav-link">GPU Access</a>
        <a href="#configuration" class="nav-link">Configuration</a>
        <a href="#benefits" class="nav-link">Benefits</a>
    </nav>
    <div class="main-content-wrapper">
        <main>
        <section id="overview" class="section">
            <h2>NVIDIA Container Toolkit Overview</h2>
            <p>
                The NVIDIA Container Toolkit enables containerized applications to access NVIDIA GPUs, providing 
                GPU acceleration for AI and HPC workloads running in containers. Standard containers operate in 
                isolation from the host system and, by default, cannot access GPU hardware. The Container Toolkit 
                bridges this gap by configuring the container runtime to allow GPU passthrough, enabling containers 
                to leverage GPU acceleration seamlessly.
            </p>
        </section>

        <section id="containers" class="section">
            <h2>Standard Container Limitations</h2>
            <p>
                Standard containers provide application isolation and portability but operate without direct 
                access to GPU hardware. This limitation means containerized AI workloads would run on CPU only, 
                resulting in significantly slower performance. For AI applications that require GPU acceleration, 
                containers need a mechanism to access GPU resources while maintaining container isolation and 
                portability.
            </p>

            <div class="mermaid">
                graph TD
                    A[Standard Container<br/>Ordinary World] --> B[No GPU Access<br/>Limited Power]
                    B --> C[CPU Only<br/>Slow Processing]
                    C --> D[Needs GPU Access<br/>Container Toolkit]
                    
                    style A fill:#7B1FA2
                    style B fill:#9C27B0
                    style C fill:#BA68C8
                    style D fill:#CE93D8
            </div>

            <div class="example-box">
                <strong>GPU Access Challenge:</strong>
                Without GPU access, containerized AI workloads are limited to CPU processing, which is orders 
                of magnitude slower than GPU acceleration. Training a neural network that takes hours on GPU 
                could take days or weeks on CPU. The NVIDIA Container Toolkit solves this by enabling containers 
                to access GPU hardware while maintaining container benefits like isolation, portability, and 
                resource management.
            </div>

            <div class="mermaid">
                mindmap
                  root((Container Toolkit))
                    Installation
                      Package Installation
                        nvidia-container-toolkit
                        Runtime Configuration
                      Runtime Setup
                        Docker Integration
                        containerd Support
                        CRI-O Support
                    GPU Access
                      Device Passthrough
                        --gpus all
                        --gpus device=0
                        Specific GPU Selection
                      Environment Variables
                        NVIDIA_VISIBLE_DEVICES
                        CUDA_VISIBLE_DEVICES
                      Resource Management
                        GPU Allocation
                        Memory Limits
                    Runtime Configuration
                      nvidia-ctk
                        Runtime Configure
                        GPU Discovery
                      Container Runtime
                        Docker Daemon
                        containerd Config
                      Device Plugins
                        Kubernetes Integration
                        GPU Scheduling
                    Benefits
                      Performance
                        GPU Acceleration
                        High Throughput
                      Isolation
                        Container Isolation
                        Resource Limits
                      Portability
                        Run Anywhere
                        Consistent Environment
                    Use Cases
                      Training
                        Distributed Training
                        Multi-GPU Jobs
                      Inference
                        Model Serving
                        Batch Processing
                      Development
                        Local Development
                        CI/CD Pipelines
            </div>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>Without Container Toolkit</th>
                        <th>With Container Toolkit</th>
                        <th>Impact</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>GPU Access</strong></td>
                        <td>No GPU access, CPU only</td>
                        <td>Full GPU access</td>
                        <td>100x+ performance improvement</td>
                    </tr>
                    <tr>
                        <td><strong>Training Time</strong></td>
                        <td>Days or weeks</td>
                        <td>Hours</td>
                        <td>10-100x faster</td>
                    </tr>
                    <tr>
                        <td><strong>Isolation</strong></td>
                        <td>N/A (no containers)</td>
                        <td>Full container isolation</td>
                        <td>Better security and resource management</td>
                    </tr>
                    <tr>
                        <td><strong>Portability</strong></td>
                        <td>Host-dependent</td>
                        <td>Container portable</td>
                        <td>Run anywhere with GPU</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section id="toolkit" class="section">
            <h2>Container Toolkit Functionality</h2>
            <p>
                The NVIDIA Container Toolkit provides the necessary components to enable GPU access from containers. 
                It includes runtime configuration tools, GPU device drivers, and libraries that allow containers 
                to discover and utilize GPU resources. The toolkit integrates with popular container runtimes 
                including Docker, containerd, and CRI-O, making GPU access seamless for containerized applications.
            </p>

            <div class="mermaid">
                sequenceDiagram
                    participant User
                    participant Docker
                    participant Toolkit
                    participant GPU
                    participant Container
                    
                    User->>Docker: docker run --gpus all
                    Docker->>Toolkit: Request GPU Access
                    Toolkit->>GPU: Discover Available GPUs
                    GPU-->>Toolkit: GPU Device Info
                    Toolkit->>Docker: Configure GPU Passthrough
                    Docker->>Container: Start with GPU Access
                    Container->>GPU: Execute CUDA Operations
                    GPU-->>Container: Return Results
                    
                    Note over Toolkit,GPU: Seamless GPU Access
            </div>

            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 30px 0;">
                <div class="portal-card" style="border-left: 5px solid #7B1FA2;">
                    <h3 style="color: #7B1FA2; margin-bottom: 15px;">GPU Gateway</h3>
                    <p><strong>Function:</strong> Connects containers to GPUs</p>
                    <p style="margin-top: 10px; font-size: 0.9rem; color: #666;">
                        Creates the bridge between container runtime and GPU hardware.
                    </p>
                </div>

                <div class="portal-card" style="border-left: 5px solid #9C27B0;">
                    <h3 style="color: #9C27B0; margin-bottom: 15px;">GPU Passthrough</h3>
                    <p><strong>Function:</strong> Enables GPU access</p>
                    <p style="margin-top: 10px; font-size: 0.9rem; color: #666;">
                        Allows containers to directly access GPU devices.
                    </p>
                </div>

                <div class="portal-card" style="border-left: 5px solid #BA68C8;">
                    <h3 style="color: #BA68C8; margin-bottom: 15px;">Resource Management</h3>
                    <p><strong>Function:</strong> Controls GPU allocation</p>
                    <p style="margin-top: 10px; font-size: 0.9rem; color: #666;">
                        Manages which GPUs are available to which containers.
                    </p>
                </div>

                <div class="portal-card" style="border-left: 5px solid #CE93D8;">
                    <h3 style="color: #CE93D8; margin-bottom: 15px;">Seamless Access</h3>
                    <p><strong>Function:</strong> Transparent GPU access</p>
                    <p style="margin-top: 10px; font-size: 0.9rem; color: #666;">
                        Makes GPU access as simple as using CPU resources.
                    </p>
                </div>
            </div>

            <div class="mermaid">
                flowchart LR
                    A[Container] -->|Request GPU| B[Container Toolkit]
                    B --> C[GPU Gateway]
                    C --> D[GPU Passthrough]
                    D --> E[GPU Hardware]
                    
                    B --> F[Resource Management]
                    F --> E
                    
                    E --> G[GPU Acceleration<br/>Available]
                    
                    style A fill:#7B1FA2
                    style B fill:#9C27B0
                    style E fill:#BA68C8
                    style G fill:#CE93D8
            </div>

            <div class="portal-card">
                <h3>What the Toolkit Does</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Creates Gateway:</strong> Connects containers to GPUs</li>
                    <li><strong>Enables Access:</strong> Allows GPU passthrough</li>
                    <li><strong>Manages Resources:</strong> Controls GPU allocation</li>
                    <li><strong>Provides Seamless Access:</strong> Makes GPU access transparent</li>
                </ul>
            </div>

            <div class="code-example">
# Example: Using Container Toolkit
# 1. Install Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
    sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker

# 2. Run container with GPU access
docker run --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# 3. Run with specific GPU
docker run --gpus '"device=0"' nvidia/cuda:11.8.0-base-ubuntu22.04 python train.py

# 4. Run with multiple GPUs
docker run --gpus all -e NVIDIA_VISIBLE_DEVICES=0,1,2,3 nvidia/cuda:11.8.0-base-ubuntu22.04 python train.py
            </div>
        </section>

        <section id="gpu-resources" class="section">
            <h2>GPU Resources and Acceleration</h2>
            <p>
                GPU resources provide high-performance acceleration for AI workloads. GPUs offer 
                thousands of parallel cores, enabling massive parallelism for neural network operations. 
                This acceleration dramatically improves training and inference performance compared 
                to CPU-only execution.
            </p>

            <div class="mermaid">
                graph TB
                    A[GPU Resources] --> B[High Performance<br/>100x+ Speedup]
                    A --> C[Parallel Processing<br/>Thousands of Cores]
                    A --> D[AI Acceleration<br/>Neural Networks]
                    A --> E[Optimized Performance<br/>Tensor Operations]
                    
                    B --> F[Training Acceleration]
                    C --> F
                    D --> F
                    E --> F
                    
                    style A fill:#7B1FA2
                    style F fill:#9C27B0
            </div>

            <div class="example-box">
                <strong>GPU Acceleration Benefits:</strong>
                GPU resources provide significant acceleration for AI workloads. Training that takes 
                days on CPU can complete in hours on GPU. Inference latency reduces from seconds to 
                milliseconds. GPUs are optimized for parallel matrix operations and neural network 
                computations, making them essential for AI workloads.
            </div>
        </section>

        <section id="runtime-config" class="section">
            <h2>Runtime Configuration</h2>
            <p>
                Runtime configuration sets up the container runtime to enable GPU access. The Container 
                Toolkit modifies the runtime configuration (Docker, containerd) to allow GPU passthrough, 
                enabling containers to access GPU resources.
            </p>

            <div class="mermaid">
                sequenceDiagram
                    participant Admin
                    participant Toolkit
                    participant Runtime
                    participant GPU
                    participant Container
                    
                    Admin->>Toolkit: nvidia-ctk runtime configure
                    Toolkit->>Runtime: Modify Configuration
                    Runtime->>GPU: Enable Passthrough
                    Container->>Runtime: Request GPU
                    Runtime->>GPU: Allocate Resources
                    GPU->>Container: Provide Access
                    
                    Note over Admin,Container: Runtime configured<br/>for GPU access
            </div>

            <div class="portal-card">
                <h3>Configuration Process</h3>
                <p>
                    The command <code>nvidia-ctk runtime configure</code> modifies the container runtime 
                    configuration to enable GPU passthrough. It updates runtime settings (e.g., Docker 
                    daemon.json, containerd config.toml) to include GPU device access. Once configured, 
                    containers can request and access GPU resources through the runtime.
                </p>
            </div>
        </section>

        <section id="passthrough" class="section">
            <h2>GPU Passthrough Mechanism</h2>
            <p>
                GPU passthrough enables containers to access GPU devices directly. The Container Toolkit 
                handles device mapping, driver compatibility, and resource allocation, making GPU access 
                seamless for containers without requiring changes to application code.
            </p>

            <div class="mermaid">
                graph TB
                    A[Container Request] --> B[Runtime Intercepts]
                    B --> C[Container Toolkit]
                    C --> D[GPU Device Mapping]
                    D --> E[Driver Compatibility]
                    E --> F[GPU Access Granted]
                    
                    F --> G[Container Uses GPU]
                    G --> H[AI Acceleration]
                    
                    style A fill:#7B1FA2
                    style C fill:#9C27B0
                    style F fill:#BA68C8
                    style H fill:#CE93D8
            </div>

            <div class="example-box">
                <strong>Passthrough Process:</strong>
                GPU passthrough works by mapping GPU devices into containers. When a container requests 
                GPU access, the Container Toolkit maps the GPU device files (/dev/nvidia*) into the 
                container namespace. The container can then use GPU libraries (CUDA, cuDNN) as if the 
                GPU were directly attached. The Toolkit handles driver compatibility and device 
                management transparently.
            </div>
        </section>

        <section id="performance" class="section">
            <h2>Performance Benefits</h2>
            <p>
                Enabling GPU access in containers provides significant performance improvements for AI 
                workloads. GPU acceleration dramatically reduces training time and inference latency 
                compared to CPU-only execution.
            </p>

            <div class="mermaid">
                graph TB
                    A[Performance Comparison] --> B[CPU Only<br/>Baseline]
                    A --> C[GPU Access<br/>Accelerated]
                    
                    B --> D[Training: Days]
                    B --> E[Inference: Seconds]
                    
                    C --> F[Training: Hours<br/>10-100x Faster]
                    C --> G[Inference: Milliseconds<br/>100-1000x Faster]
                    
                    style B fill:#7B1FA2
                    style C fill:#9C27B0
                    style F fill:#BA68C8
                    style G fill:#CE93D8
            </div>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>CPU Only</th>
                        <th>GPU Access</th>
                        <th>Improvement</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Training Time</strong></td>
                        <td>Days to weeks</td>
                        <td>Hours to days</td>
                        <td>10-100x faster</td>
                    </tr>
                    <tr>
                        <td><strong>Inference Latency</strong></td>
                        <td>Seconds</td>
                        <td>Milliseconds</td>
                        <td>100-1000x faster</td>
                    </tr>
                    <tr>
                        <td><strong>Throughput</strong></td>
                        <td>Low</td>
                        <td>High</td>
                        <td>Significant increase</td>
                    </tr>
                    <tr>
                        <td><strong>Resource Efficiency</strong></td>
                        <td>CPU-bound</td>
                        <td>GPU-accelerated</td>
                        <td>Better utilization</td>
                    </tr>
                </tbody>
            </table>

            <div class="example-box">
                <strong>Performance Impact:</strong>
                When containers access GPUs, performance improvements are substantial:
                <ul style="margin-top: 10px; padding-left: 20px;">
                    <li><strong>Training:</strong> Reduces from days to hours (10-100x speedup)</li>
                    <li><strong>Inference:</strong> Reduces from seconds to milliseconds (100-1000x speedup)</li>
                    <li><strong>Efficiency:</strong> Better GPU resource utilization, higher throughput</li>
                    <li><strong>Scalability:</strong> Run more workloads concurrently with GPU acceleration</li>
                </ul>
            </div>
        </section>
            </main>

            </div>
</div>

    <footer>
        <p><strong>NVIDIA Container Toolkit</strong> - GPU Access for Containers</p>
        <p>Enable GPU acceleration in containerized AI workloads</p>
    </footer>

    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
        document.querySelectorAll('.nav-link').forEach(link => {
            link.addEventListener('click', function(e) {
                e.preventDefault();
                document.querySelectorAll('.nav-link').forEach(l => l.classList.remove('active'));
                this.classList.add('active');
                document.querySelector(this.getAttribute('href')).scrollIntoView({ behavior: 'smooth' }
);
            });
        });
        window.addEventListener('scroll', () => {
            const sections = document.querySelectorAll('.section');
            const navLinks = document.querySelectorAll('.nav-link');
            let current = '';
            sections.forEach(s => {
                if (window.pageYOffset >= s.offsetTop - 100) current = s.getAttribute('id');
            }
);
            navLinks.forEach(l => {
                l.classList.remove('active');
                if (l.getAttribute('href') === `#${current}`) l.classList.add('active');
            });
        });
    </script>
</body>
</html>
