<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NCCL: NVIDIA Collective Communications Library for Multi-GPU Systems</title>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
:root {
    --primary-color: #7B1FA2;
    --secondary-color: #9C27B0;
    --accent-color: #BA68C8;
    --bg-color: #F3E5F5;
    --card-bg: #FFFFFF;
    --text-color: #1A1A1A;
    --border-color: #CE93D8;
    --shadow: 0 2px 8px rgba(123, 31, 162, 0.1);
}
body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', sans-serif;
    background: var(--bg-color);
    color: var(--text-color);
    line-height: 1.6;
}
header {
    text-align: center;
    padding: 50px 40px;
    background: linear-gradient(135deg, #7B1FA2, #9C27B0);
    color: white;
    position: relative;
}
header::before {
    content: '';
    position: absolute;
    font-size: 200px;
    opacity: 0.1;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
}
header h1 {
    font-size: 2.5rem;
    font-weight: 700;
    margin-bottom: 12px;
    text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
}
.sidebar-wrapper {
    display: flex;
    min-height: calc(100vh - 200px);
}
.nav-menu {
    width: 280px;
    min-width: 280px;
    background: #80DEEA;
    border-right: 2px solid var(--border-color);
    padding: 20px 0;
    position: sticky;
    top: 0;
    height: calc(100vh - 200px);
    overflow-y: auto;
    display: flex;
    flex-direction: column;
    gap: 4px;
}
.nav-link {
    color: var(--text-color);
    text-decoration: none;
    padding: 12px 20px;
    margin: 0 10px;
    border-radius: 6px;
    transition: all 0.2s ease;
    font-weight: 500;
    display: block;
}
.nav-link:hover, .nav-link.active {
    background: var(--primary-color);
    color: white;
}
.main-content-wrapper {
    flex: 1;
    width: calc(100% - 280px);
    min-width: 0;
}

.section {
    padding: 40px;
    min-height: calc(100vh - 200px);
    border-bottom: 1px solid var(--border-color);
    max-width: 100%;
}
.section h2 {
    font-size: 2.2rem;
    margin-bottom: 20px;
    color: var(--primary-color);
    border-bottom: 3px solid var(--primary-color);
    padding-bottom: 10px;
}
.orchestra-card {
    background: var(--card-bg);
    padding: 25px;
    border-radius: 12px;
    border: 2px solid var(--border-color);
    margin: 20px 0;
    box-shadow: var(--shadow);
}
.mermaid { margin: 30px 0; text-align: center; }
.example-box {
    background: linear-gradient(135deg, #F3E5F5, #CE93D8);
    border-left: 5px solid var(--primary-color);
    padding: 20px 25px;
    margin: 20px 0;
    border-radius: 8px;
}
footer {
    text-align: center;
    padding: 40px;
    background: #CE93D8;
    border-top: 2px solid var(--border-color);
    color: var(--text-color);
}
@media (max-width: 768px) {
    header h1 { font-size: 2rem; }
    
    .nav-menu {
        width: 100%;
        min-width: 100%;
        height: auto;
        position: relative;
        flex-direction: row;
        flex-wrap: wrap;
        padding: 10px;
        border-right: none;
        border-bottom: 2px solid var(--border-color);
    }
    .nav-link {
        padding: 8px 15px;
        margin: 4px;
    }
    
    .section { padding: 25px 20px; }
}
    .section {
    padding: 40px;
    min-height: calc(100vh - 200px);
    border-bottom: 1px solid var(--border-color);
    max-width: 100%;
}
    
.author-credit {
    font-size: 1rem;
    font-weight: 600;
    margin-top: 15px;
    padding: 8px 20px;
    background: rgba(255, 255, 255, 0.25);
    border-radius: 6px;
    display: inline-block;
    text-shadow: 1px 1px 2px rgba(0,0,0,0.3);
    border: 2px solid rgba(255, 255, 255, 0.4);
}
</style>
</head>
<body>
    <header>
        <h1>23. NCCL: NVIDIA Collective Communications Library</h1>
        <p class="subtitle">Multi-GPU Communication for Distributed AI Training</p>
        <p class="author-credit">Explained by Naman Adep</p>
    </header>

    <div class="sidebar-wrapper">
    <nav class="nav-menu">
        <a href="#overview" class="nav-link active">Overview</a>
        <a href="#gpus" class="nav-link">Multi-GPU Systems</a>
        <a href="#operations" class="nav-link">Collective Operations</a>
        <a href="#synchronization" class="nav-link">Synchronization</a>
        <a href="#training" class="nav-link">Distributed Training</a>
    </nav>
    <div class="main-content-wrapper">
        <main>
        <section id="overview" class="section">
            <h2>NCCL Overview</h2>
            <p>
                NCCL (NVIDIA Collective Communications Library) is a library of communication primitives 
                optimized for multi-GPU and multi-node systems. It enables efficient collective operations 
                such as all-reduce, all-gather, and broadcast across multiple GPUs, which are essential for 
                distributed AI training. NCCL is optimized for NVIDIA GPUs and high-performance interconnects 
                like InfiniBand and NVLink, providing the communication layer needed for scalable distributed 
                training.
            </p>

            <div class="mermaid">
                graph TD
                    A[NCCL<br/>Orchestra] --> B[Coordinates GPUs<br/>Musicians]
                    B --> C[Collective Operations<br/>Playing Together]
                    C --> D[Harmony<br/>Synchronized]
                    
                    style A fill:#7B1FA2
                    style B fill:#9C27B0
                    style C fill:#BA68C8
                    style D fill:#CE93D8
            </div>

            <div class="example-box">
                <strong>Collective Communication:</strong>
                NCCL coordinates multiple GPUs to perform collective operations together. Each GPU processes 
                its portion of the workload, and NCCL ensures synchronized communication and data exchange 
                between all GPUs, enabling efficient distributed training.
            </div>

            <div class="mermaid">
                mindmap
                  root((NCCL))
                    Collective Operations
                      All-Reduce
                      All-Gather
                      Broadcast
                      Reduce-Scatter
                    GPU Coordination
                      Multi-GPU Systems
                      Multi-Node Systems
                      Synchronization
                    Communication
                      InfiniBand Support
                      NVLink Support
                      RDMA
                    Benefits
                      Efficient Training
                      Scalable Communication
                      Optimized Performance
            </div>
        </section>

        <section id="gpus" class="section">
            <h2>GPU Coordination</h2>
            <p>
                In distributed training, each GPU processes its portion of the workload. NCCL coordinates 
                communication between GPUs, ensuring they work together effectively to train the model.
            </p>

            <div class="mermaid">
                graph TB
                    A[NCCL<br/>Coordination] --> B[GPU 1<br/>Process Data]
                    A --> C[GPU 2<br/>Process Data]
                    A --> D[GPU 3<br/>Process Data]
                    A --> E[GPU N<br/>Process Data]
                    
                    B --> F[Collective Operations]
                    C --> F
                    D --> F
                    E --> F
                    
                    style A fill:#7B1FA2
                    style F fill:#9C27B0
            </div>

            <div class="mermaid">
                sequenceDiagram
                    participant GPU1
                    participant GPU2
                    participant GPU3
                    participant NCCL
                    
                    GPU1->>GPU1: Compute Gradients
                    GPU2->>GPU2: Compute Gradients
                    GPU3->>GPU3: Compute Gradients
                    
                    GPU1->>NCCL: All-Reduce Request
                    GPU2->>NCCL: All-Reduce Request
                    GPU3->>NCCL: All-Reduce Request
                    
                    NCCL->>NCCL: Aggregate Gradients
                    NCCL-->>GPU1: Return Aggregated Gradients
                    NCCL-->>GPU2: Return Aggregated Gradients
                    NCCL-->>GPU3: Return Aggregated Gradients
                    
                    Note over GPU1,GPU3: Synchronized Gradient Update
            </div>

            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 30px 0;">
                <div class="orchestra-card" style="border-left: 5px solid #7B1FA2;">
                    <h3 style="color: #7B1FA2; margin-bottom: 15px;">Data Processing</h3>
                    <p><strong>Function:</strong> Process training data</p>
                    <p style="margin-top: 10px; font-size: 0.9rem; color: #666;">
                        Each GPU processes its portion of the training dataset independently.
                    </p>
                </div>

                <div class="orchestra-card" style="border-left: 5px solid #9C27B0;">
                    <h3 style="color: #9C27B0; margin-bottom: 15px;">Synchronization</h3>
                    <p><strong>Function:</strong> Coordinate operations</p>
                    <p style="margin-top: 10px; font-size: 0.9rem; color: #666;">
                        NCCL ensures all GPUs synchronize for collective operations.
                    </p>
                </div>

                <div class="orchestra-card" style="border-left: 5px solid #BA68C8;">
                    <h3 style="color: #BA68C8; margin-bottom: 15px;">Communication</h3>
                    <p><strong>Function:</strong> Exchange data</p>
                    <p style="margin-top: 10px; font-size: 0.9rem; color: #666;">
                        GPUs exchange gradients and model parameters during training.
                    </p>
                </div>
            </div>

            <div class="mermaid">
                flowchart TD
                    A[Distributed Training] --> B[GPU 1<br/>Process Batch 1]
                    A --> C[GPU 2<br/>Process Batch 2]
                    A --> D[GPU 3<br/>Process Batch 3]
                    A --> E[GPU N<br/>Process Batch N]
                    
                    B --> F[Compute Gradients]
                    C --> F
                    D --> F
                    E --> F
                    
                    F --> G[NCCL<br/>All-Reduce]
                    G --> H[Synchronized Gradients<br/>All GPUs]
                    
                    H --> I[Update Model<br/>Continue Training]
                    
                    style A fill:#7B1FA2
                    style G fill:#9C27B0
                    style I fill:#BA68C8
            </div>

            <div class="orchestra-card">
                <h3>GPU Roles</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>Each GPU:</strong> Processes its portion of the training data</li>
                    <li><strong>Coordination:</strong> NCCL ensures synchronized operations</li>
                    <li><strong>Communication:</strong> GPUs exchange gradients and data</li>
                    <li><strong>Synchronization:</strong> All GPUs coordinate for collective operations</li>
                </ul>
            </div>
        </section>

        <section id="operations" class="section">
            <h2>Collective Operations</h2>
            <p>
                Collective operations define how GPUs communicate and synchronize data. Each operation 
                specifies the pattern of data exchange between GPUs.
            </p>

            <div class="mermaid">
                flowchart TD
                    A[Collective Operations] --> B[AllReduce<br/>All GPUs Combine]
                    A --> C[AllGather<br/>All GPUs Collect]
                    A --> D[Broadcast<br/>One to All]
                    A --> E[Reduce<br/>All to One]
                    A --> F[AllToAll<br/>All Exchange]
                    
                    B --> G[Gradient Synchronization]
                    C --> H[Data Collection]
                    D --> I[Model Distribution]
                    E --> J[Result Aggregation]
                    F --> K[Data Exchange]
                    
                    style A fill:#7B1FA2
                    style G fill:#9C27B0
            </div>

            <div class="orchestra-card">
                <h3>Types of Collective Operations</h3>
                <ul style="margin-top: 15px; padding-left: 20px; line-height: 2;">
                    <li><strong>AllReduce:</strong> All GPUs combine their results</li>
                    <li><strong>AllGather:</strong> All GPUs collect data from all others</li>
                    <li><strong>Broadcast:</strong> One GPU sends data to all others</li>
                    <li><strong>Reduce:</strong> Combine data from all GPUs to one</li>
                    <li><strong>AllToAll:</strong> All GPUs exchange data with all others</li>
                </ul>
            </div>

            <div class="mermaid">
                graph TD
                    A[Collective Operations] --> B[AllReduce<br/>Combine Results]
                    A --> C[AllGather<br/>Collect Data]
                    A --> D[Broadcast<br/>Distribute Data]
                    A --> E[Reduce<br/>Combine to One]
                    
                    style A fill:#7B1FA2
            </div>
        </section>

        <section id="synchronization" class="section">
            <h2>Synchronization</h2>
            <p>
                Synchronization ensures all GPUs coordinate their operations at the right time, enabling 
                efficient collective communication and preventing data races or inconsistencies.
            </p>

            <div class="mermaid">
                sequenceDiagram
                    participant NCCL
                    participant GPU1
                    participant GPU2
                    participant GPU3
                    
                    NCCL->>GPU1: Start Operation
                    NCCL->>GPU2: Start Operation
                    NCCL->>GPU3: Start Operation
                    GPU1->>GPU2: Exchange Data
                    GPU2->>GPU3: Exchange Data
                    GPU3->>GPU1: Exchange Data
                    Note over GPU1,GPU3: Synchronized<br/>Operation
            </div>

            <div class="example-box">
                <strong>Synchronization Importance:</strong>
                NCCL requires all GPUs to synchronize their operations for collective communication 
                to work correctly. When synchronization is perfect, collective operations are efficient 
                and fast, enabling high-performance distributed training.
            </div>
        </section>

        <section id="training" class="section">
            <h2>Distributed Training</h2>
            <p>
                Distributed training uses multiple GPUs working together to train a model. NCCL 
                coordinates communication between GPUs, enabling efficient gradient synchronization 
                and model updates across the cluster.
            </p>

            <div class="mermaid">
                graph LR
                    A[Distributed Training<br/>Performance] --> B[Data Parallelism<br/>Different Data]
                    A --> C[Model Parallelism<br/>Different Layers]
                    B --> D[AllReduce Gradients<br/>Synchronize]
                    C --> D
                    D --> E[Update Model<br/>Beautiful Result]
                    
                    style A fill:#7B1FA2
                    style B fill:#9C27B0
                    style C fill:#BA68C8
                    style D fill:#CE93D8
                    style E fill:#E1BEE7
            </div>

            <div class="example-box">
                <strong>Training Efficiency:</strong>
                When NCCL coordinates distributed training effectively, all GPUs work together 
                efficiently, gradients are synchronized optimally, and the model trains faster than 
                any single GPU could achieve alone. This demonstrates the power of collective 
                communication for scaling AI training.
            </div>
        </section>
            </main>

            </div>
</div>

    <footer>
        <p><strong>NCCL: The Orchestra</strong> - Collective Communication for Multi-GPU Systems</p>
        <p>When GPUs play together, the performance is beautiful! </p>
    </footer>

    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
        document.querySelectorAll('.nav-link').forEach(link => {
            link.addEventListener('click', function(e) {
                e.preventDefault();
                document.querySelectorAll('.nav-link').forEach(l => l.classList.remove('active'));
                this.classList.add('active');
                document.querySelector(this.getAttribute('href')).scrollIntoView({ behavior: 'smooth' }
);
            });
        });
        window.addEventListener('scroll', () => {
            const sections = document.querySelectorAll('.section');
            const navLinks = document.querySelectorAll('.nav-link');
            let current = '';
            sections.forEach(s => {
                if (window.pageYOffset >= s.offsetTop - 100) current = s.getAttribute('id');
            }
);
            navLinks.forEach(l => {
                l.classList.remove('active');
                if (l.getAttribute('href') === `#${current}`) l.classList.add('active');
            });
        });
    </script>
</body>
</html>
